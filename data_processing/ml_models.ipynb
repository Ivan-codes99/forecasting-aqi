{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CAP 5768: Introduction to Data Science**\n",
    "## Final Project Program: Implementing Machine Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned = pd.read_csv('cleaned_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE) of Linear Regression: 5.2664\n",
      "R-Squared (R^2) of Linear Regression: 0.8709\n"
     ]
    }
   ],
   "source": [
    "# Define features and target\n",
    "X = data_cleaned[['NO2 AQI', 'O3 AQI', 'SO2 AQI', 'CO AQI']]\n",
    "y = data_cleaned['Overall AQI']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the scalar\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# LinearRegression fit\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make a prediction\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "lr_mse = mean_absolute_error(y_test, y_pred_lr)\n",
    "lr_r2 = r2_score(y_test, y_pred_lr)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) of Linear Regression: {lr_mse:.4f}\")\n",
    "print(f\"R-Squared (R^2) of Linear Regression: {lr_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE) of Random Forest Regressor: 36.8555\n",
      "R^2 Score (R^2) of Random Forest Regressor: 0.9089\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Regressor\n",
    "forest_model = RandomForestRegressor(n_estimators=100, random_state=42, max_depth=4, min_samples_split=20)\n",
    "\n",
    "# Train model\n",
    "forest_model.fit(X_train, y_train)\n",
    "\n",
    "# Make a prediction\n",
    "y_pred_forest = forest_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "forest_mse = mean_squared_error(y_test, y_pred_forest)\n",
    "forest_r2 = r2_score(y_test, y_pred_forest)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) of Random Forest Regressor: {forest_mse:.4f}\")\n",
    "print(f\"R^2 Score (R^2) of Random Forest Regressor: {forest_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE) of Ridge Regression: 36.8555\n",
      "R^2 Score (R^2) of Ridge Regression: 0.9089\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regression\n",
    "ridge_model = Ridge(alpha=1.0, fit_intercept=True, solver='auto')\n",
    "\n",
    "# Train model\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "# Make a prediction\n",
    "y_pred_ridge = ridge_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "ridge_mse = mean_squared_error(y_test, y_pred_ridge)\n",
    "ridge_r2 = r2_score(y_test, y_pred_ridge)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) of Ridge Regression: {forest_mse:.4f}\")\n",
    "print(f\"R^2 Score (R^2) of Ridge Regression: {forest_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE) of XGBoost Regressor: 2.5558\n",
      "R^2 Score (R^2) of XGBoost Regressor: 0.9937\n"
     ]
    }
   ],
   "source": [
    "# Gradicent Boosting Regressor\n",
    "xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "\n",
    "# Fit model\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make a prediction\n",
    "y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "xgb_mse = mean_squared_error(y_test, y_pred_xgb)\n",
    "xgb_r2 = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) of XGBoost Regressor: {xgb_mse:.4f}\")\n",
    "print(f\"R^2 Score (R^2) of XGBoost Regressor: {xgb_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Analysis\n",
    "* While all models performed within an acceptable range, the XGBoost Regressor model emerged as the top performer. This is evident when analyzing both its R-squared score and its Mean Squared Error (MSE) compared to the other models. The results indicate that it accurately predicted 99.37% of the variance in the data, as shown by the R-squared score of 0.9937. Furthermore, it exhibited a relatively small error margin, with a Mean Squared Error of 2.558, which is notably good for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for XGBoost Regressor: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'subsample': 1.0}\n",
      "Best score for XGBoost Regressor: -2.2736\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters grid for XGBoost\n",
    "param_grid_xgb = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "# Perform Grid Search with 5-fold cross-validation\n",
    "grid_search_xgb = GridSearchCV(estimator=xgb_model, param_grid=param_grid_xgb, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the grid search\n",
    "grid_search_xgb.fit(X_train_scaled, y_train.values.ravel())\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best parameters for XGBoost Regressor:\", grid_search_xgb.best_params_)\n",
    "print(f\"Best score for XGBoost Regressor: {grid_search_xgb.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE) of Best XGBoost Regressor: 2.3270\n",
      "R^2 Score (R^2) of Best XGBoost Regressor: 0.9942\n"
     ]
    }
   ],
   "source": [
    "best_xgb_model = grid_search_xgb.best_estimator_\n",
    "\n",
    "y_pred_best_xgb = best_xgb_model.predict(X_test_scaled)\n",
    "\n",
    "best_xgb_mse = mean_squared_error(y_test, y_pred_best_xgb)\n",
    "best_xgb_r2 = r2_score(y_test, y_pred_best_xgb)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) of Best XGBoost Regressor: {best_xgb_mse:.4f}\")\n",
    "print(f\"R^2 Score (R^2) of Best XGBoost Regressor: {best_xgb_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Analysis\n",
    "* To identify the optimal parameters for our top-performing model, XGBoost, we conducted a Grid Search utilizing 5-fold cross-validation. The parameters identified through this process allowed for a slight improvement in the model's performance.\n",
    "* Ideally, we would have liked to further expand the Grid Search by incorporating additional parameters such as reg_alpha and reg_lambda, among others. However, time constraints limited our ability to perform this more extensive search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
